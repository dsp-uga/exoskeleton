{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhyper params: \\n    activation functions\\n    number of layers\\n    number of neurons per layer \\n    optimizer\\n    learning rate\\n    epochs\\n    batch size \\n    unit_forget_bias\\n'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# Pre-processing the data #\n",
    "###########################\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "###########################\n",
    "# Pre-processing the data #\n",
    "###########################\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import scale\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense,BatchNormalization\n",
    "from keras.layers import LSTM, Input\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "\n",
    "debug=False\n",
    "files = ['miller_idle', 'sam_wild', 'layton_idle', 'miller_wild', 'val_idle', 'layton_wild', 'sam_idle', 'val_wild']  #filename in the folder\n",
    "STANDARD_SCALER = True\n",
    "PROGRAMMATIC = True\n",
    "\n",
    "#params for the net:\n",
    "layers   = [30, 10]\n",
    "dropouts = [.2, .2]\n",
    "activations = ['relu','relu']#*4#['relu','relu','relu','relu']\n",
    "recurrent_dropouts = [.2,.2]\n",
    "epochs = 30 \n",
    "batch_size = 10\n",
    "unit_forget_bias = True\n",
    "#Initializers: he_normal, glorot_normal\n",
    "kernel_initializer = 'lecun_uniform'\n",
    "recurrent_initializer = 'lecun_uniform'\n",
    "go_backwards = True\n",
    "LSTM_FLAG = True\n",
    "\n",
    "'''\n",
    "hyper params: \n",
    "    activation functions\n",
    "    number of layers\n",
    "    number of neurons per layer \n",
    "    optimizer\n",
    "    learning rate\n",
    "    epochs\n",
    "    batch size \n",
    "    unit_forget_bias\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2Callback(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        X_test, y_test = self.test_data\n",
    "        pred = self.model.predict(X_test)\n",
    "        r2 = r2_score(pred,y_test)\n",
    "        print('r2 score: {},\\n'.format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def makeLSTM(layer_list,layers,activation,dropouts):\n",
    "def makeLSTM(visible,layers,activation,dropouts):\n",
    "    assert(len(layers)==len(dropouts) and \n",
    "           len(layers)==len(activations) and \n",
    "           len(layers)==len(recurrent_dropouts) )\n",
    "    \n",
    "    cell = LSTM if LSTM_FLAG else GRU()\n",
    "    '''\n",
    "    Mimicing the following:\n",
    "    visible = ...\n",
    "    extract = LSTM(30, activation=\"relu\",dropout=0.5,return_sequences=True)(visible)\n",
    "    extract = LSTM(20, activation=\"relu\",dropout=0.5,return_sequences=True)(extract)\n",
    "    extract = LSTM(10, activation=\"relu\",dropout=0.5,return_sequences=True)(extract)\n",
    "    extract = LSTM(5, activation=\"relu\",dropout=0.5,return_sequences=True)(extract)\n",
    "    '''\n",
    "    info = zip(layers,dropouts,activations)\n",
    "    for i in range(len(layers)): #layer,dropout,activation in info:\n",
    "        if debug: \n",
    "            print(\"layer: {},dropout: {}, activation: {}\".format(layers[i],dropouts[i],activations[i]))\n",
    "        #visible = LSTM(layers[i],\n",
    "        visible = cell(layers[i],\n",
    "                       activation = activations[i], \n",
    "                       dropout = dropouts[i],\n",
    "                       recurrent_dropout = recurrent_dropouts[i],\n",
    "                       return_sequences=True, \n",
    "                       unit_forget_bias=unit_forget_bias,\n",
    "                       kernel_initializer = kernel_initializer,\n",
    "                       recurrent_initializer = recurrent_initializer)(visible)\n",
    "        #layer_list += LSTM(layers[i],activation = activations[i], dropout = dropouts[i], return_sequences=True)(layer_list[-1])\n",
    "    #return layer_list[-1]\n",
    "    return visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler should be a scaling object: either StandardScaler or MinMaxScaler\n",
    "def preprocessing(filename,scaler):\n",
    "    \n",
    "    d = pd.read_csv(filename)   #read the CSV file\n",
    "    if debug:\n",
    "        print(\"Original {}\".format(d))\n",
    "        print (d)\n",
    "    d_drop = d.drop(\"Sensor type\", axis=1)   #drop the first column -- sensor type\n",
    "    if debug:\n",
    "        print (\"Dropped Sensor Type {}\".format(d_drop))\n",
    "    d_matrix = d_drop.as_matrix()   # Using pandas again to convert the revised table into a matrix\n",
    "\n",
    "    n_array = np.array(d_matrix) # now we use numpy to convert the pandas matrix into a Numpy array\n",
    "\n",
    "    #delete the last row if the revised row number is odd, otherwise cut last too rows\n",
    "    n_array = n_array[:-1] if len(n_array) % 2 == 1 else n_array[:-2] \n",
    "    if debug:\n",
    "        print (\"Delete odd rows \\n{}\".format(n_array[:16,:]))\n",
    "\n",
    "    #Concatenate the EMG and Accerlaration rows for (nearly) the same time moments\n",
    "    n_conc = np.concatenate((n_array[::2], n_array[1::2]), axis = 1)    \n",
    "    if debug:\n",
    "        print (\"Concatenated array \\n{}\".format(n_conc[:16,:]))\n",
    "\n",
    "    n_del = np.delete(n_conc, [1, 2], axis = 1) #delete the second and third columns of each list in the matrix\n",
    "    if debug:\n",
    "        print (\"Delete Zero rows \\n{}\".format(n_del[:16,:]))\n",
    "        print(\"n_del.shape: {}\".format(n_del.shape))\n",
    "        \n",
    "    targets = targets = n_del[1:, 2:5] #target for data t is [x,y,z] of t+1\n",
    "    data = n_del[:-1] # no target for last example, so drop it.\n",
    "    \n",
    "    if debug: \n",
    "        print(\"data.shape: {}\".format(data.shape))\n",
    "    #Normalize the data using the supplied scaler\n",
    "    data = scaler.fit(data).transform(data)\n",
    "    targets = scaler.fit(targets).transform(targets)\n",
    "        \n",
    "    return data, targets\n",
    "\n",
    "def save_array(array, filename):\n",
    "    np.savetxt(filename + 'processed', array, delimiter=',', fmt = '%f')  #save the output into a file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_attributes = 6\n",
    "num_targets = 3\n",
    "\n",
    "scaler = StandardScaler() if STANDARD_SCALER else MinMaxScaler()\n",
    "\n",
    "data = np.empty((0,num_attributes)) #initializing empty array with proper shape\n",
    "targets = np.empty((0,num_targets)) #empty array for targets\n",
    "\n",
    "#collecting all the data into one set\n",
    "for f in files:\n",
    "    processed_data, processed_targets = preprocessing(f,scaler)\n",
    "    data = np.append(data, processed_data, axis=0)\n",
    "    targets = np.append(targets, processed_targets, axis=0)\n",
    "    save_array(processed_data, f)\n",
    "    \n",
    "\n",
    "#data = scale(data)\n",
    "\n",
    "if debug : print(\"data[0]: {}\".format(data[0]))\n",
    "temp = list()\n",
    "for i in range(1,targets.shape[0]):\n",
    "    #temp = targets[i]-targets[i-1]\n",
    "    temp.append(targets[i]-targets[i-1])\n",
    "input_data = list()\n",
    "for i in range(0,data.shape[0]):\n",
    "    input_data.append([data[i]])\n",
    "    \n",
    "input_data = np.array(data)\n",
    "acceleration_targets = np.array(temp)\n",
    "data = data.reshape((data.shape[0],1,data.shape[1]))[1:]\n",
    "if debug : print(\"data[0:10]: {}\".format(data[0:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, y_train, X_test, y_test = data[0:40000],acceleration_targets[0:40000],data[40000:],acceleration_targets[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_32 (InputLayer)        (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 1, 30)             4440      \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 1, 10)             1640      \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 3)                 168       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 12        \n",
      "=================================================================\n",
      "Total params: 6,260\n",
      "Trainable params: 6,260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 40000 samples, validate on 34121 samples\n",
      "Epoch 1/30\n",
      "40000/40000 [==============================] - 34s 857us/step - loss: 0.1712 - mean_absolute_error: 0.1897 - val_loss: 0.1497 - val_mean_absolute_error: 0.1584\n",
      "r2 score: -36.24799289060085,\n",
      "\n",
      "Epoch 2/30\n",
      "40000/40000 [==============================] - 32s 802us/step - loss: 0.1630 - mean_absolute_error: 0.1898 - val_loss: 0.1507 - val_mean_absolute_error: 0.1635\n",
      "r2 score: -14.775596234654993,\n",
      "\n",
      "Epoch 3/30\n",
      "40000/40000 [==============================] - 33s 824us/step - loss: 0.1615 - mean_absolute_error: 0.1895 - val_loss: 0.1487 - val_mean_absolute_error: 0.1649\n",
      "r2 score: -7.854793443451098,\n",
      "\n",
      "Epoch 4/30\n",
      "40000/40000 [==============================] - 31s 784us/step - loss: 0.1573 - mean_absolute_error: 0.1885 - val_loss: 0.1520 - val_mean_absolute_error: 0.1680\n",
      "r2 score: -4.8185549573816155,\n",
      "\n",
      "Epoch 5/30\n",
      "40000/40000 [==============================] - 31s 778us/step - loss: 0.1568 - mean_absolute_error: 0.1883 - val_loss: 0.1598 - val_mean_absolute_error: 0.1720\n",
      "r2 score: -3.1616393940300083,\n",
      "\n",
      "Epoch 6/30\n",
      "40000/40000 [==============================] - 31s 780us/step - loss: 0.1552 - mean_absolute_error: 0.1872 - val_loss: 0.1600 - val_mean_absolute_error: 0.1741\n",
      "r2 score: -2.886876731444339,\n",
      "\n",
      "Epoch 7/30\n",
      "40000/40000 [==============================] - 31s 779us/step - loss: 0.1556 - mean_absolute_error: 0.1874 - val_loss: 0.1557 - val_mean_absolute_error: 0.1707\n",
      "r2 score: -4.012836669283801,\n",
      "\n",
      "Epoch 8/30\n",
      "40000/40000 [==============================] - 31s 779us/step - loss: 0.1549 - mean_absolute_error: 0.1864 - val_loss: 0.1669 - val_mean_absolute_error: 0.1802\n",
      "r2 score: -2.664378769346241,\n",
      "\n",
      "Epoch 9/30\n",
      "40000/40000 [==============================] - 31s 780us/step - loss: 0.1540 - mean_absolute_error: 0.1864 - val_loss: 0.1695 - val_mean_absolute_error: 0.1796\n",
      "r2 score: -2.5925408026775947,\n",
      "\n",
      "Epoch 10/30\n",
      "40000/40000 [==============================] - 31s 778us/step - loss: 0.1547 - mean_absolute_error: 0.1856 - val_loss: 0.1684 - val_mean_absolute_error: 0.1791\n",
      "r2 score: -3.8875434333811945,\n",
      "\n",
      "Epoch 11/30\n",
      "40000/40000 [==============================] - 31s 776us/step - loss: 0.1552 - mean_absolute_error: 0.1864 - val_loss: 0.1926 - val_mean_absolute_error: 0.1938\n",
      "r2 score: -2.5821826289763066,\n",
      "\n",
      "Epoch 12/30\n",
      "40000/40000 [==============================] - 31s 778us/step - loss: 0.1548 - mean_absolute_error: 0.1865 - val_loss: 0.1857 - val_mean_absolute_error: 0.1884\n",
      "r2 score: -3.3448832566674707,\n",
      "\n",
      "Epoch 13/30\n",
      "40000/40000 [==============================] - 31s 781us/step - loss: 0.1528 - mean_absolute_error: 0.1859 - val_loss: 0.1691 - val_mean_absolute_error: 0.1829\n",
      "r2 score: -3.540692856720117,\n",
      "\n",
      "Epoch 14/30\n",
      "40000/40000 [==============================] - 31s 774us/step - loss: 0.1528 - mean_absolute_error: 0.1853 - val_loss: 0.1890 - val_mean_absolute_error: 0.1940\n",
      "r2 score: -2.0544432794498566,\n",
      "\n",
      "Epoch 15/30\n",
      "40000/40000 [==============================] - 31s 781us/step - loss: 0.1518 - mean_absolute_error: 0.1851 - val_loss: 0.1797 - val_mean_absolute_error: 0.1850\n",
      "r2 score: -2.895603417210141,\n",
      "\n",
      "Epoch 16/30\n",
      "40000/40000 [==============================] - 31s 775us/step - loss: 0.1530 - mean_absolute_error: 0.1850 - val_loss: 0.1995 - val_mean_absolute_error: 0.1975\n",
      "r2 score: -2.264355076982541,\n",
      "\n",
      "Epoch 17/30\n",
      "40000/40000 [==============================] - 31s 778us/step - loss: 0.1521 - mean_absolute_error: 0.1840 - val_loss: 0.1786 - val_mean_absolute_error: 0.1904\n",
      "r2 score: -3.2142758031419656,\n",
      "\n",
      "Epoch 18/30\n",
      "40000/40000 [==============================] - 31s 775us/step - loss: 0.1537 - mean_absolute_error: 0.1853 - val_loss: 0.1737 - val_mean_absolute_error: 0.1872\n",
      "r2 score: -3.193070822544391,\n",
      "\n",
      "Epoch 19/30\n",
      "40000/40000 [==============================] - 31s 772us/step - loss: 0.1509 - mean_absolute_error: 0.1844 - val_loss: 0.2054 - val_mean_absolute_error: 0.1968\n",
      "r2 score: -2.6220768610409313,\n",
      "\n",
      "Epoch 20/30\n",
      "40000/40000 [==============================] - 31s 774us/step - loss: 0.1519 - mean_absolute_error: 0.1848 - val_loss: 0.1782 - val_mean_absolute_error: 0.1861\n",
      "r2 score: -3.266826124529123,\n",
      "\n",
      "Epoch 21/30\n",
      "40000/40000 [==============================] - 32s 790us/step - loss: 0.1513 - mean_absolute_error: 0.1847 - val_loss: 0.1895 - val_mean_absolute_error: 0.1976\n",
      "r2 score: -2.380319696280255,\n",
      "\n",
      "Epoch 22/30\n",
      "40000/40000 [==============================] - 34s 851us/step - loss: 0.1514 - mean_absolute_error: 0.1846 - val_loss: 0.1967 - val_mean_absolute_error: 0.1931\n",
      "r2 score: -1.976801982291671,\n",
      "\n",
      "Epoch 23/30\n",
      "39950/40000 [============================>.] - ETA: 0s - loss: 0.1508 - mean_absolute_error: 0.1836"
     ]
    }
   ],
   "source": [
    "#https://machinelearningmastery.com/keras-functional-api-deep-learning/\n",
    "visible = Input(shape=(1,6))\n",
    "# feature extraction\n",
    "\n",
    "\n",
    "\n",
    "if PROGRAMMATIC: \n",
    "# classification output\n",
    "    #layer = makeLSTM([visible],layers,activations,dropouts)\n",
    "    layer = makeLSTM(visible,layers,activations,dropouts)\n",
    "    class11 = LSTM(3,activation=None)(layer)\n",
    "    output1 = Dense(3, activation='linear')(class11)\n",
    "    model = Model(inputs=visible, outputs=output1)\n",
    "\n",
    "else:\n",
    "    extract = LSTM(30, activation=\"relu\",dropout=0.5,return_sequences=True)(visible)\n",
    "    extract = LSTM(20, activation=\"relu\",dropout=0.5,return_sequences=True)(extract)\n",
    "    extract = LSTM(10, activation=\"relu\",dropout=0.5,return_sequences=True)(extract)\n",
    "    extract = LSTM(5, activation=\"relu\",dropout=0.5,return_sequences=True)(extract)\n",
    "    class11 = LSTM(3,activation=None)(extract)\n",
    "    output1 = Dense(3, activation='linear')(class11)\n",
    "    model = Model(inputs=visible, outputs=output1)\n",
    "\n",
    "# summarize layers\n",
    "print(model.summary())\n",
    "#sgd= SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n",
    "model.compile(loss='mean_squared_error',optimizer=\"adam\",metrics=[\"mae\"])\n",
    "model.fit(X_train,y_train,verbose=1,batch_size=batch_size,epochs=epochs,validation_data=(X_test,y_test),callbacks=[R2Callback((X_test,y_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "[[[ 0.03710197  1.62231395 -0.10282867  1.92876797 -0.51208322\n",
      "    1.62231101]]\n",
      "\n",
      " [[-0.00751908  1.62254565 -0.1113402   1.52475057 -0.4092953\n",
      "    1.62254271]]\n",
      "\n",
      " [[-0.03870005  1.62275628 -0.09431714  1.39036525 -0.29127953\n",
      "    1.62275335]]\n",
      "\n",
      " [[-0.06934342  1.62298798 -0.07729409  1.08713684 -0.31792825\n",
      "    1.62298505]]\n",
      "\n",
      " [[-0.09514836  1.62324074 -0.1113402   0.95275152 -0.33315609\n",
      "    1.62323781]]\n",
      "\n",
      " [[-0.07579466  1.62345138 -0.11985173  1.01994418 -0.37883962\n",
      "    1.62346951]]\n",
      "\n",
      " [[ 0.10430234  1.62368307 -0.13687479  1.35590748 -0.52731106\n",
      "    1.62368014]]\n",
      "\n",
      " [[ 0.11344159  1.62389371 -0.1709209   1.42396158 -0.4663997\n",
      "    1.62389078]]\n",
      "\n",
      " [[ 0.06183171  1.62412541 -0.15389784  1.35590748 -0.37883962\n",
      "    1.62412247]]\n",
      "\n",
      " [[ 0.01774826  1.62433604 -0.1709209   1.22152216 -0.24559601\n",
      "    1.62433311]]]\n",
      "\n",
      "predictions:\n",
      "[[ 0.00028317 -0.02860187  0.06666152]\n",
      " [ 0.00766453 -0.01773301  0.06156803]\n",
      " [ 0.01276261 -0.01676627  0.05601323]\n",
      " [ 0.01365463 -0.00872439  0.06249333]\n",
      " [ 0.01394914 -0.00466128  0.06544323]\n",
      " [ 0.01282431 -0.00572742  0.06752317]\n",
      " [ 0.00564988 -0.01204072  0.07133132]\n",
      " [ 0.00685819 -0.01428162  0.06828305]\n",
      " [ 0.01013583 -0.01397836  0.06401504]\n",
      " [ 0.01645157 -0.01388782  0.05723295]]\n",
      "\n",
      "targets:\n",
      "[[-0.00851181 -0.40402364  0.10278835]\n",
      " [ 0.01702362 -0.13438739  0.11801626]\n",
      " [ 0.01702362 -0.30323309 -0.02664883]\n",
      " [-0.03404724 -0.13438739 -0.0152279 ]\n",
      " [-0.00851181  0.0671937  -0.04568371]\n",
      " [-0.01702362  0.33596848 -0.14847207]\n",
      " [-0.03404724  0.06805515  0.06091162]\n",
      " [ 0.01702362 -0.06805515  0.08756045]\n",
      " [-0.01702362 -0.13438739  0.13324416]\n",
      " [ 0.         -0.30237163  0.03045581]]\n"
     ]
    }
   ],
   "source": [
    "print('data:')\n",
    "print(X_test[:10])\n",
    "print('\\npredictions:')\n",
    "print(model.predict(X_test)[:10])\n",
    "print('\\ntargets:')\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "[]\n",
      "\n",
      "predictions:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-17136a1d7470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\npredictions:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\ntargets:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reg' is not defined"
     ]
    }
   ],
   "source": [
    "print('data:')\n",
    "print(X_test[:10, 2:5])\n",
    "print('\\npredictions:')\n",
    "print(reg.predict(X_test)[:10])\n",
    "print('\\ntargets:')\n",
    "print(y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
